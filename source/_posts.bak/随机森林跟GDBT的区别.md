---
title: 随机森林跟GDBT的区别
comments: true
toc: true
mathjax2: true
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,classification]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2017-05-09 19:45:26
---
决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型容易展示。单决策树又有一些不好的地方，比如说容易over-fitting。多个决策树协同决策，能够有效提升模型的预测精度。
 <!--more-->
## bagging
1. 基于bootstrap sampling 自助采样法，重复性有放回的随机采用部分样本进行训练最后再将结果 voting 或者 averaging 。
2. 并行式算法
3. 每一层大约花费时间相同
4. 主要关注于降低方差，但是不能降低偏差

## 随机森林
在bagging的基础上，加入随机属性选择机制。
1. 对于回归模型，选择属性数量，建议选择全部属性的三分之一
2. 对于分类模型选择呞性数量，建议选择全部属性的平方根
3. 随机森林于高维数据鏆的处理能力很，宁可以处理成千上的输入变量，并确最要的变量，因此被认为瘯一个帍错的降维斻法。
4. 高度并行匶，易ຎ剆布式实现
5. 在解决回归问题时并没有它在分类中变现那么好，主膁因ฺ它不能给出一个连廭的输出
6. 无法⎧父模型内郸的运行，等于盒子
7. 调参方法：网搜索
8. n_estimators越多越好，但计算量寽壞
9. 随机森林不进졌剪枝À儳篖树剪枝是因为防楢迧拟合，蠌随机.林的“随望”已滏防止了拟合，因此不錀要剪枝。
## GBLT
GBDT中的树都☯回彚栙，不是分类树 ，因为gradient boost y要按獟礱嗽敐的梯度近似拟合残差ﴌȿ样拟合犄是连续数值，因此只有回归树。
Gradient Boosting-，每个新的稱型的建立是为了使得之卌戡型的残差往梯e方向减少，与传统Boosting对正确、错误样本进行加权有着很大的区别。

## XGBoost
单独再说吧

## 随机森林与GBDT区别
相同点：
（1）都是由多棵树组成的，都是集成学习算法
（2）最终的结果都是由多颗树一起决定

不同点：
(1)组成随机森林的树可以是分类树，也可以是回归树，但是GBDT只能由回归树组成。
(2)组成随机森林的树可以并行生成，但是组成GBDT的树只能串行生成。
(3)对于最终的输出结果，随机森林采用多数投票；而GBDT是将所有的结果累加起来，或者加权起来
(4)随机森林对异常值不敏感，而GBDT对异常值非常敏感
(5)随机森林通过减小方差来提高性能，GBDT通过减小偏差来提高性能
